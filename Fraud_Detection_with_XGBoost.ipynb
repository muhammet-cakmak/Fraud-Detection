{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\nimport gc\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":1,"outputs":[{"output_type":"stream","text":"/kaggle/input/ieee-fraud-detection/sample_submission.csv\n/kaggle/input/ieee-fraud-detection/test_transaction.csv\n/kaggle/input/ieee-fraud-detection/test_identity.csv\n/kaggle/input/ieee-fraud-detection/train_identity.csv\n/kaggle/input/ieee-fraud-detection/train_transaction.csv\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"**FUNCTIONS**"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"def reduce_mem_usage(df):\n    \"\"\" iterate through all the columns of a dataframe and modify the data type\n        to reduce memory usage.        \n    \"\"\"\n    start_mem = df.memory_usage().sum() / 1024**2\n    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n    \n    for col in df.columns:\n        col_type = df[col].dtype\n        \n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n        else:\n            df[col] = df[col].astype('category')\n\n    end_mem = df.memory_usage().sum() / 1024**2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n    \n    return df\n\ndef import_data(file):\n    \"\"\"create a dataframe and optimize its memory usage\"\"\"\n    df = pd.read_csv(file, parse_dates=True, keep_date_col=True, index_col='TransactionID')\n    df = reduce_mem_usage(df)\n    return df\n\n# AMAÇ: seçilen değişkenlerin uids lere göre gruplanıp ortalamaların alıp yeni değişkene atamak.\ndef aggreg(columns, userid, aggr='mean'):\n    \n    for col in columns:\n        new_col_name = col+'_'+userid+'_'+aggr # sectigi kolon isimlerini ve aggregation degerini birleştirip yeni değişken ismi oluşturmuş\n        df_temp = pd.concat([X_train[[userid, col]], X_test[[userid,col]]]) # Train ve Test setten seçtiği değişkenleri alt alta birleştirmiş.\n        df_temp.loc[df_temp[col]==-1,col] = np.nan # main_colums dan gelen değişkende -1 olan değerleri nan yapar.\n        # col değişkenine göre groupby atmış ve col değişkenine agg_type türüne göre işlem yapmış ve bunu new_col_name isimli yeni değişkene atamış.İNDEX İ DE SIFIRLADI.\n        df_temp = df_temp.groupby(userid)[col].agg([aggr]).reset_index().rename(columns={aggr: new_col_name})\n        df_temp.index = list(df_temp[userid]) # df_temp nin sıfırlanan indexleri col değişkeninin index leriyle değiştirildi.\n        df_temp = df_temp[new_col_name].to_dict()  # Yni oluşturulan değişken sözlük türüne çevirildi.\n        # Bu yeni değerler Train ve Test setlerinde ilgili kolona(col değişkeni) karşılık gelecek şekilde  \"new_col_name\" ismiyle Train ve Test sete eklendi.\n        X_train[new_col_name] = X_train[userid].map(df_temp).astype('float32')\n        X_test[new_col_name]  = X_test[userid].map(df_temp).astype('float32')\n        # Yeni olusturulan degiskenlerdeki nan değerler yerine -1 yazdırır.\n        X_train[new_col_name].fillna(-1,inplace=True)\n        X_test[new_col_name].fillna(-1,inplace=True)\n      \n\n # AMAÇ: uids lere göre main_colums daki değişkenler gruplanır ve ve bu main_column daki farklı değerler sayılır ve \n# bu toplam sayı Test ve Trainde ilgili \"TransactionID\" nin karşısına atanır.\ndef aggreg_uniq(columns, userid):\n    for col in columns:  \n        df = pd.concat([X_train[[userid,col]],X_test[[userid,col]]],axis=0)\n        #Burada uniq aynı col değişkenine denk gelen col değişkenindeki farklı değerlerin toplam sayısıdır.\n        uniq = df.groupby(userid)[col].agg(['nunique'])['nunique'].to_dict()\n\n        # \"uid_P_emaildomain_ct\" şeklinde değişken oluşturulur ve Train ile test setinde col değişkenindeki değerlere \"TransactionID\" baz alınarak uniq değerleri eşlenir.\n        X_train[col+'_count'] = X_train[userid].map(uniq).astype('float32')\n        X_test[col+'_count'] = X_test[userid].map(uniq).astype('float32')\n        \n# Her degeri minumum deger kadar arttiriyoruz, boylece hic eksi deger kalmiyor ve minumum deger 0 oluyor, bunu yapmadaki amac, NAN degere -1\n# verdigimizde ayri bir sinif olarak algilayabilmesi, cunku hic negatif deger kalmadi, sadece nan olanlar negatif olmus oldu\ndef num_positiv(X_train,X_test):\n    for f in X_train.columns:  \n        # Bütün nümerik değerleri pozitif yap ve NAN değerleri -1 yap. ['TransactionAmt','TransactionDT'] kolonları hariç.\n        if f not in ['TransactionAmt','TransactionDT',\"isFraud\"]: \n            mn = np.min((X_train[f].min(),X_test[f].min()))  # X_train ve X_test deki f kolonunun minimum degerlerini kıyasla ve en küçük olanını \"mn\" ye ata.\n\n            # Buradaki amaç bütün değerlerden en küçük değeri çıkartarak onları pozitif yaparken aralarındaki değer farkınıda korumaktır.\n            X_train[f] -= np.float32(mn)   # X_train deki f kolonundaki değerlerden mn yi yani en küçük değeri çıkarır.\n            X_test[f] -= np.float32(mn)    # X_test deki f kolonundaki değerlerden mn yi yani en küçük değeri çıkarır.\n            X_train[f].fillna(-1,inplace=True)  # X_train deki NaN değerleri -1 ile doldurur.\n            X_test[f].fillna(-1,inplace=True)   # X_test deki NaN değerleri -1 ile doldurur.\n            \n# AMAÇ: encode_FE fonksiyonu girilen data setlerindeki belirtilen kolonları normalize edip türlerini \"float32\" ye çevirip _FE uzantılı yeni bir değişken olarak data setlerine ekler.\ndef class_freq(cols):\n    for col in cols:\n        df = pd.concat([X_train[col],X_test[col]])\n        vc = df.value_counts(dropna=True).to_dict() # col. kolonundaki unique değerleri alıp bunları normalize ediyor ve listeye çevirip vc değişkeninde saklıyor.\n        vc[-1] = -1  # vc.  sözlüğüne -1 key adı ile -1 değerini ekliyor.\n        nm = col+'_freq' # kolon isimlerine uyguladığı FE encode uzantısını ekliyor.\n        X_train[nm] = X_train[col].map(vc)  #vc deki keys değerleri ile df1[col] daki index değerlerini eşleyip karşılığına vc deki values değerlerini atayıp bunu df1'e yeni değişken olarak atar.\n        X_test[nm] = X_test[col].map(vc)  #vc deki keys değerleri ile df2[col] daki index değerlerini eşleyip karşılığına vc deki values değerlerini atayıp bunu df2'ye yeni değişken olarak atar.\n        del df; x=gc.collect()\n        \n# butun categoruc (object) degiskenler icin factorize islemi yapildi,\n# factorize ile label encoder arasindaki en onemli fark, factorize nan degerleri -1 olarak tutuyor ama labelencoder da nan degerleri onceden doldurmak gerekiyor\ndef factorize_categoric():    \n    for col in X_train.select_dtypes(include=['category','object']).columns:\n        df = pd.concat([X_train[col],X_test[col]])\n        df,_ = df.factorize(sort=True)\n        X_train[col] = df[:len(X_train)].astype('int32')\n        X_test[col] = df[len(X_train):].astype('int32')\n        del df; x=gc.collect()        \n        \n# TEK BIR USERID OLUSTURMAK ICIN ONCELIKLE USERID OLUSTURMAK ICIN FAYDALI OLDUGUNU DEGERLENDIRDIGIMIZ FEATURELARI BELIRLEYIP\n# O FEATURLER ARASINDAN FARKLI KOMPINASYONLARI DENEYIP BIRDEN COK USERID OLUSTURALIM, DAHA SONRA BUNLARIN KOLERASYONUNA BAKIP\n# MANTIKLI OLAN HANGISI ISE ONU USERID OLARAK BELIRLEYELIM\n\ndef user_id(col1,col2):\n    us_id = col1+'_'+col2\n    \n    X_train[us_id] = X_train[col1].astype(str)+'_'+X_train[col2].astype(str)   # 12926.0_215.0, 3663.0_230.0  şeklinde çıktı üretir.\n    X_test[us_id] = X_test[col1].astype(str)+'_'+X_test[col2].astype(str)\n   \n","execution_count":2,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\nprint('Loading data...')\n\ntrain_id = import_data(\"../input/ieee-fraud-detection/train_identity.csv\")\nprint('\\tSuccessfully loaded train_identity!')\n\nX_train = import_data('../input/ieee-fraud-detection/train_transaction.csv')\nprint('\\tSuccessfully loaded train_transaction!')\nX_train = X_train.merge(train_id, how='left', left_index=True, right_index=True) # Train setini kendi içinde merge etmiş\n\ntest_id = import_data('../input/ieee-fraud-detection/test_identity.csv')\nprint('\\tSuccessfully loaded test_identity!')\n\nX_test = import_data('../input/ieee-fraud-detection/test_transaction.csv')\nprint('\\tSuccessfully loaded test_transaction!')\n\ntest_id.columns = train_id.columns\nX_test = X_test.merge(test_id, how='left', left_index=True, right_index=True)  # Test setini kendi içinde merge etmiş\n\npd.set_option('max_columns', None)\n\n# TARGET\ny_train = X_train['isFraud'].copy()  # Train deki bağımlı değişkeni y_train setine atamış.\n\nprint('Data was successfully loaded!\\n')","execution_count":3,"outputs":[{"output_type":"stream","text":"Loading data...\nMemory usage of dataframe is 45.12 MB\nMemory usage after optimization is: 10.57 MB\nDecreased by 76.6%\n\tSuccessfully loaded train_identity!\nMemory usage of dataframe is 1775.15 MB\nMemory usage after optimization is: 489.41 MB\nDecreased by 72.4%\n\tSuccessfully loaded train_transaction!\nMemory usage of dataframe is 44.39 MB\nMemory usage after optimization is: 10.40 MB\nDecreased by 76.6%\n\tSuccessfully loaded test_identity!\nMemory usage of dataframe is 1519.24 MB\nMemory usage after optimization is: 427.17 MB\nDecreased by 71.9%\n\tSuccessfully loaded test_transaction!\nData was successfully loaded!\n\nCPU times: user 2min 4s, sys: 1min 55s, total: 4min\nWall time: 4min 1s\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Nan değerlere göre değişkenleri gruplama\nnan_groups={}\nv_cols = ['V'+str(i) for i in range(1,340)]\nfor i in X_train.columns:\n    nan_sum = X_train[i].isna().sum()\n    try:\n        nan_groups[nan_sum].append(i)\n    except:\n        nan_groups[nan_sum]=[i]\n\nfor i,j in nan_groups.items():\n    print('The Sum of the NaN Values =',i)\n    print(j)\n    \n    \n\nnon_group_list=list()\nfor i,j in nan_groups.items():\n    if len(j)>5:\n        if i != 0:\n            non_group_list.append(i)\n            \n            \n# Grupların kendi içinde korelasyon değeri 0.70 ten yüksek olan değişken grupları\n\n# D1 - V11 \ngrp1 = [[1],[2,3],[4,5],[6,7],[8,9],[10,11]]\n# V12 - V34\ngrp2 = [[12,13],[14],[15,16,17,18,21,22,31,32,33,34],[19,20],[23,24],[25,26],[27,28],[29,30]]\n# V35 - V52\ngrp3 = [[35,36],[37,38],[39,40,42,43,50,51,52],[41],[44,45],[46,47],[48,49]]\n# V53 - V74\ngrp4 = [[53,54],[55,56],[57,58,59,60,63,64,71,72,73,74],[61,62],[65],[66,67],[68],[69,70]]\n# V74 - V94\ngrp5 = [[75,76],[77,78],[79,80,81,84,85,92,93,94],[82,83],[86,87],[88],[89],[90,91]]\n# V95 - V107\ngrp6 = [[95,96,97,101,102,103,105,106],[98],[99,100],[104]]\n# V107 - V123\ngrp7 = [[107],[108,109,110,114],[111,112,113],[115,116],[117,118,119],[120,122],[121],[123]]\n# V124 - V137\ngrp8 = [[124,125],[126,127,128,132,133,134],[129],[130,131],[135,136,137]]\n# V138 - V163\ngrp9 = [[138],[139,140],[141,142],[146,147],[148,149,153,154,156,157,158],[161,162,163]]\n# V167 - V183\ngrp10 = [[167,168,177,178,179],[172,176],[173],[181,182,183]]\n# V184 - V216\ngrp11 = [[186,187,190,191,192,193,196,199],[202,203,204,211,212,213],[205,206],[207],[214,215,216]]\n# V217 - V238\ngrp12 = [[217,218,219,231,232,233,236,237],[223],[224,225],[226],[228],[229,230],[235]]\n# V240 - V262\ngrp13 = [[240,241],[242,243,244,258],[246,257],[247,248,249,253,254],[252],[260],[261,262]]\n# V263 - V278\ngrp14 = [[263,265,264],[266,269],[267,268],[273,274,275],[276,277,278]]\n# V220 - V272\ngrp15 = [[220],[221,222,227,245,255,256,259],[234],[238,239],[250,251],[270,271,272]]\n# V279 - V299\ngrp16 = [[279,280,293,294,295,298,299],[284],[285,287],[286],[290,291,292],[297]]\n# V302 - V321\ngrp17 = [[302,303,304],[305],[306,307,308,316,317,318],[309,311],[310,312],[319,320,321]]\n# V281 V315\ngrp18 = [[281],[282,283],[288,289],[296],[300,301],[313,314,315]]\n# V322 - V339\ngrp19 = [[322,323,324,326,327,328,329,330,331,332,333],[325],[334,335,336],[337,338,339]]\n\ngrp_list = [grp1,grp2,grp3,grp4,grp5,grp6,grp7,grp8,grp9,grp10,grp11,grp12,grp13,grp14,grp15,grp16,grp17,grp18,grp19]\n\n\n\n\n# Aynı korelasyondaki değişkenlerden unique değeri fazla olanı seçen fonksiyon\ndef clip_group(group,df):\n    clipped_list = []\n    for i in group:\n        maximum = 0; \n        V_num = i[0]\n        for j in i:\n            n = df['V'+str(j)].value_counts().count()\n            if n>maximum:\n                maximum = n\n                V_num = j\n            \n        clipped_list.append(V_num)\n    \n        \n    print('Variables in the clipped_list: ',clipped_list)\n    return clipped_list\n\n\n# Korelasyon sonucunda modelde kullanılmasına karar verilen  V değişkenleri V_clipped_cols değişkeninde tutulmuştur.\nV_clipped_cols = list()\nfor i in grp_list:\n    for j in clip_group(i,X_train):\n        V_clipped_cols.append(\"V\"+str(j))\n        \n\nfor i in range (1, 339):\n    name = \"V\"+str(i)\n    if name not in V_clipped_cols:\n        X_train.drop(\"V\"+str(i),axis=1, inplace=True)\n        X_test.drop(\"V\"+str(i),axis=1, inplace=True)","execution_count":4,"outputs":[{"output_type":"stream","text":"The Sum of the NaN Values = 0\n['isFraud', 'TransactionDT', 'TransactionAmt', 'ProductCD', 'card1', 'C1', 'C2', 'C3', 'C4', 'C5', 'C6', 'C7', 'C8', 'C9', 'C10', 'C11', 'C12', 'C13', 'C14']\nThe Sum of the NaN Values = 8933\n['card2']\nThe Sum of the NaN Values = 1565\n['card3']\nThe Sum of the NaN Values = 1577\n['card4']\nThe Sum of the NaN Values = 4259\n['card5']\nThe Sum of the NaN Values = 1571\n['card6']\nThe Sum of the NaN Values = 65706\n['addr1', 'addr2']\nThe Sum of the NaN Values = 352271\n['dist1']\nThe Sum of the NaN Values = 552913\n['dist2']\nThe Sum of the NaN Values = 94456\n['P_emaildomain']\nThe Sum of the NaN Values = 453249\n['R_emaildomain']\nThe Sum of the NaN Values = 1269\n['D1', 'V281', 'V282', 'V283', 'V288', 'V289', 'V296', 'V300', 'V301', 'V313', 'V314', 'V315']\nThe Sum of the NaN Values = 280797\n['D2']\nThe Sum of the NaN Values = 262878\n['D3']\nThe Sum of the NaN Values = 168922\n['D4']\nThe Sum of the NaN Values = 309841\n['D5']\nThe Sum of the NaN Values = 517353\n['D6']\nThe Sum of the NaN Values = 551623\n['D7']\nThe Sum of the NaN Values = 515614\n['D8', 'D9', 'id_09', 'id_10']\nThe Sum of the NaN Values = 76022\n['D10']\nThe Sum of the NaN Values = 279287\n['D11', 'V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10', 'V11']\nThe Sum of the NaN Values = 525823\n['D12']\nThe Sum of the NaN Values = 528588\n['D13']\nThe Sum of the NaN Values = 528353\n['D14']\nThe Sum of the NaN Values = 89113\n['D15']\nThe Sum of the NaN Values = 271100\n['M1', 'M2', 'M3']\nThe Sum of the NaN Values = 281444\n['M4']\nThe Sum of the NaN Values = 350482\n['M5']\nThe Sum of the NaN Values = 169360\n['M6']\nThe Sum of the NaN Values = 346265\n['M7']\nThe Sum of the NaN Values = 346252\n['M8', 'M9']\nThe Sum of the NaN Values = 76073\n['V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18', 'V19', 'V20', 'V21', 'V22', 'V23', 'V24', 'V25', 'V26', 'V27', 'V28', 'V29', 'V30', 'V31', 'V32', 'V33', 'V34']\nThe Sum of the NaN Values = 168969\n['V35', 'V36', 'V37', 'V38', 'V39', 'V40', 'V41', 'V42', 'V43', 'V44', 'V45', 'V46', 'V47', 'V48', 'V49', 'V50', 'V51', 'V52']\nThe Sum of the NaN Values = 77096\n['V53', 'V54', 'V55', 'V56', 'V57', 'V58', 'V59', 'V60', 'V61', 'V62', 'V63', 'V64', 'V65', 'V66', 'V67', 'V68', 'V69', 'V70', 'V71', 'V72', 'V73', 'V74']\nThe Sum of the NaN Values = 89164\n['V75', 'V76', 'V77', 'V78', 'V79', 'V80', 'V81', 'V82', 'V83', 'V84', 'V85', 'V86', 'V87', 'V88', 'V89', 'V90', 'V91', 'V92', 'V93', 'V94']\nThe Sum of the NaN Values = 314\n['V95', 'V96', 'V97', 'V98', 'V99', 'V100', 'V101', 'V102', 'V103', 'V104', 'V105', 'V106', 'V107', 'V108', 'V109', 'V110', 'V111', 'V112', 'V113', 'V114', 'V115', 'V116', 'V117', 'V118', 'V119', 'V120', 'V121', 'V122', 'V123', 'V124', 'V125', 'V126', 'V127', 'V128', 'V129', 'V130', 'V131', 'V132', 'V133', 'V134', 'V135', 'V136', 'V137']\nThe Sum of the NaN Values = 508595\n['V138', 'V139', 'V140', 'V141', 'V142', 'V146', 'V147', 'V148', 'V149', 'V153', 'V154', 'V155', 'V156', 'V157', 'V158', 'V161', 'V162', 'V163']\nThe Sum of the NaN Values = 508589\n['V143', 'V144', 'V145', 'V150', 'V151', 'V152', 'V159', 'V160', 'V164', 'V165', 'V166']\nThe Sum of the NaN Values = 450909\n['V167', 'V168', 'V172', 'V173', 'V176', 'V177', 'V178', 'V179', 'V181', 'V182', 'V183', 'V186', 'V187', 'V190', 'V191', 'V192', 'V193', 'V196', 'V199', 'V202', 'V203', 'V204', 'V205', 'V206', 'V207', 'V211', 'V212', 'V213', 'V214', 'V215', 'V216']\nThe Sum of the NaN Values = 450721\n['V169', 'V170', 'V171', 'V174', 'V175', 'V180', 'V184', 'V185', 'V188', 'V189', 'V194', 'V195', 'V197', 'V198', 'V200', 'V201', 'V208', 'V209', 'V210']\nThe Sum of the NaN Values = 460110\n['V217', 'V218', 'V219', 'V223', 'V224', 'V225', 'V226', 'V228', 'V229', 'V230', 'V231', 'V232', 'V233', 'V235', 'V236', 'V237', 'V240', 'V241', 'V242', 'V243', 'V244', 'V246', 'V247', 'V248', 'V249', 'V252', 'V253', 'V254', 'V257', 'V258', 'V260', 'V261', 'V262', 'V263', 'V264', 'V265', 'V266', 'V267', 'V268', 'V269', 'V273', 'V274', 'V275', 'V276', 'V277', 'V278']\nThe Sum of the NaN Values = 449124\n['V220', 'V221', 'V222', 'V227', 'V234', 'V238', 'V239', 'V245', 'V250', 'V251', 'V255', 'V256', 'V259', 'V270', 'V271', 'V272']\nThe Sum of the NaN Values = 12\n['V279', 'V280', 'V284', 'V285', 'V286', 'V287', 'V290', 'V291', 'V292', 'V293', 'V294', 'V295', 'V297', 'V298', 'V299', 'V302', 'V303', 'V304', 'V305', 'V306', 'V307', 'V308', 'V309', 'V310', 'V311', 'V312', 'V316', 'V317', 'V318', 'V319', 'V320', 'V321']\nThe Sum of the NaN Values = 508189\n['V322', 'V323', 'V324', 'V325', 'V326', 'V327', 'V328', 'V329', 'V330', 'V331', 'V332', 'V333', 'V334', 'V335', 'V336', 'V337', 'V338', 'V339']\nThe Sum of the NaN Values = 446307\n['id_01', 'id_12']\nThe Sum of the NaN Values = 449668\n['id_02']\nThe Sum of the NaN Values = 524216\n['id_03', 'id_04']\nThe Sum of the NaN Values = 453675\n['id_05', 'id_06']\nThe Sum of the NaN Values = 585385\n['id_07', 'id_08']\nThe Sum of the NaN Values = 449562\n['id_11', 'id_28', 'id_29']\nThe Sum of the NaN Values = 463220\n['id_13']\nThe Sum of the NaN Values = 510496\n['id_14']\nThe Sum of the NaN Values = 449555\n['id_15', 'id_35', 'id_36', 'id_37', 'id_38']\nThe Sum of the NaN Values = 461200\n['id_16']\nThe Sum of the NaN Values = 451171\n['id_17']\nThe Sum of the NaN Values = 545427\n['id_18']\nThe Sum of the NaN Values = 451222\n['id_19']\nThe Sum of the NaN Values = 451279\n['id_20']\nThe Sum of the NaN Values = 585381\n['id_21']\nThe Sum of the NaN Values = 585371\n['id_22', 'id_23', 'id_27']\nThe Sum of the NaN Values = 585793\n['id_24']\nThe Sum of the NaN Values = 585408\n['id_25']\nThe Sum of the NaN Values = 585377\n['id_26']\nThe Sum of the NaN Values = 512975\n['id_30']\nThe Sum of the NaN Values = 450258\n['id_31']\nThe Sum of the NaN Values = 512954\n['id_32']\nThe Sum of the NaN Values = 517251\n['id_33']\nThe Sum of the NaN Values = 512735\n['id_34']\nThe Sum of the NaN Values = 449730\n['DeviceType']\nThe Sum of the NaN Values = 471874\n['DeviceInfo']\nVariables in the clipped_list:  [1, 3, 4, 6, 8, 11]\nVariables in the clipped_list:  [13, 14, 17, 20, 23, 26, 27, 30]\nVariables in the clipped_list:  [36, 37, 40, 41, 44, 47, 48]\nVariables in the clipped_list:  [54, 56, 59, 62, 65, 67, 68, 70]\nVariables in the clipped_list:  [76, 78, 80, 82, 86, 88, 89, 91]\nVariables in the clipped_list:  [96, 98, 99, 104]\nVariables in the clipped_list:  [107, 108, 111, 115, 117, 120, 121, 123]\nVariables in the clipped_list:  [124, 127, 129, 130, 136]\nVariables in the clipped_list:  [138, 139, 142, 147, 156, 162]\nVariables in the clipped_list:  [178, 176, 173, 182]\nVariables in the clipped_list:  [187, 203, 205, 207, 215]\nVariables in the clipped_list:  [218, 223, 224, 226, 228, 229, 235]\nVariables in the clipped_list:  [240, 258, 257, 253, 252, 260, 261]\nVariables in the clipped_list:  [264, 266, 267, 274, 277]\nVariables in the clipped_list:  [220, 221, 234, 238, 250, 271]\nVariables in the clipped_list:  [294, 284, 285, 286, 291, 297]\nVariables in the clipped_list:  [303, 305, 307, 309, 310, 320]\nVariables in the clipped_list:  [281, 283, 289, 296, 301, 314]\nVariables in the clipped_list:  [332, 325, 335, 338]\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# frekans sayısı 2 den az olan kartlar INVALID, çok olanlar VALID kart olarak tanimlaniyor, sonra train ve testin birinde bulunmuyorsa nan yapmış sonra invalid olanları nan yapmış\n\nvalid_card = pd.concat([X_train[['card1']], X_test[['card1']]])\nvalid_card = valid_card['card1'].value_counts()\nvalid_card_std = valid_card.values.std()\n\ninvalid_cards = valid_card[valid_card<=2]\n\nvalid_card = valid_card[valid_card>2]\nvalid_card = list(valid_card.index)\n\nX_train['card1'] = np.where(X_train['card1'].isin(X_test['card1']), X_train['card1'], np.nan)\nX_test['card1']  = np.where(X_test['card1'].isin(X_train['card1']), X_test['card1'], np.nan)\n\nX_train['card1'] = np.where(X_train['card1'].isin(valid_card), X_train['card1'], np.nan)\nX_test['card1']   = np.where(X_test['card1'].isin(valid_card), X_test['card1'], np.nan)\n\n# burda frekans sayısı 2 den az olan kartlara invalid çok olanlara valid kart demiş\n# sonra train ve testin ikisinde de bulunanları almış eğer birinde bulunmuyorsa nan yapmış\n# sonra invalid olanları nan yapmış\n\nfor col in ['card2','card3','card4','card5','card6']: \n    X_train[col] = np.where(X_train[col].isin(X_test[col]), X_train[col], np.nan)\n    X_test[col]  = np.where(X_test[col].isin(X_train[col]), X_test[col], np.nan)\n\n# train ve testin ikisinde de bulunanları almış eğer birinde bulunmuyorsa nan yapmış","execution_count":5,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# USERID belirliyoruz\ncol_1 = 'card1'\ncol_2 = 'P_emaildomain'\ncol_3 = 'addr1'\n\n\nuser_id(col_1,col_2)\nuser_id(col_1+'_'+col_2,col_3)\nX_train.drop(col_1+'_'+col_2, axis = 1, inplace=True)\nX_test.drop(col_1+'_'+col_2, axis = 1, inplace=True)\n\nus_id = col_1 + '_' + col_2 + '_' + col_3\nX_train.rename(columns={us_id: 'userid'}, inplace=True)\nX_test.rename(columns={us_id: 'userid'}, inplace=True)","execution_count":6,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# cihaz ve browser tespitinin onemli oldugu varsayimiyla yapildi...\n\nfor df in [X_train,X_test]:\n\n    df['OS_id_30'] = df['id_30'].str.split(' ', expand=True)[0]\n    df['version_id_30'] = df['id_30'].str.split(' ', expand=True)[1]\n\n    df['browser_id_31'] = df['id_31'].str.split(' ', expand=True)[0]\n    df['version_id_31'] = df['id_31'].str.split(' ', expand=True)[1]","execution_count":7,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# amt ilk halinde float16, bu sekilde std ve mean NAN oluyor, float32 yapmamiz lazim\n\nfor df in [X_train,X_test]:\n\n    df['TransactionAmt'] = df['TransactionAmt'].astype('float32')\n    df['Trans_min_std'] = (df['TransactionAmt'] - df['TransactionAmt'].mean()) / df['TransactionAmt'].std()","execution_count":8,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# lastest_browser (SON VERSIYON KONTROLU) son versiyon olanlar 1 \n\nX_train[\"lastest_browser\"] = np.zeros(X_train.shape[0])\nX_test[\"lastest_browser\"] = np.zeros(X_test.shape[0])\n\ndef setBrowser(df):\n    df.loc[df[\"id_31\"]==\"samsung browser 7.0\",'lastest_browser']=1\n    df.loc[df[\"id_31\"]==\"opera 53.0\",'lastest_browser']=1\n    df.loc[df[\"id_31\"]==\"mobile safari 10.0\",'lastest_browser']=1\n    df.loc[df[\"id_31\"]==\"google search application 49.0\",'lastest_browser']=1\n    df.loc[df[\"id_31\"]==\"firefox 60.0\",'lastest_browser']=1\n    df.loc[df[\"id_31\"]==\"edge 17.0\",'lastest_browser']=1\n    df.loc[df[\"id_31\"]==\"chrome 69.0\",'lastest_browser']=1\n    df.loc[df[\"id_31\"]==\"chrome 67.0 for android\",'lastest_browser']=1\n    df.loc[df[\"id_31\"]==\"chrome 63.0 for android\",'lastest_browser']=1\n    df.loc[df[\"id_31\"]==\"chrome 63.0 for ios\",'lastest_browser']=1\n    df.loc[df[\"id_31\"]==\"chrome 64.0\",'lastest_browser']=1\n    df.loc[df[\"id_31\"]==\"chrome 64.0 for android\",'lastest_browser']=1\n    df.loc[df[\"id_31\"]==\"chrome 64.0 for ios\",'lastest_browser']=1\n    df.loc[df[\"id_31\"]==\"chrome 65.0\",'lastest_browser']=1\n    df.loc[df[\"id_31\"]==\"chrome 65.0 for android\",'lastest_browser']=1\n    df.loc[df[\"id_31\"]==\"chrome 65.0 for ios\",'lastest_browser']=1\n    df.loc[df[\"id_31\"]==\"chrome 66.0\",'lastest_browser']=1\n    df.loc[df[\"id_31\"]==\"chrome 66.0 for android\",'lastest_browser']=1\n    df.loc[df[\"id_31\"]==\"chrome 66.0 for ios\",'lastest_browser']=1\n    return df\n\nX_train=setBrowser(X_train)\nX_test=setBrowser(X_test)","execution_count":9,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# MAIL ADRESLERININ SON KISIMLARINDAN ULKE TESPITINE YONELIK URETILEN FEATURE (com, us, mx, es, de, fr, uk, jp)\n\nus_emails = ['gmail', 'net', 'edu']\n\nfor df in [X_train,X_test]:\n    for c in ['P_emaildomain', 'R_emaildomain']:\n\n        df[c + '_suffix'] = df[c].map(lambda x: str(x).split('.')[-1])\n        df[c + '_suffix'] = df[c + '_suffix'].map(lambda x: x if str(x) not in us_emails else 'us')","execution_count":10,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Check if P_emaildomain matches R_emaildomain \n# extracts prefix\n\np = 'P_emaildomain'\nr = 'R_emaildomain'\nunknown = 'email_not_provided'\n\ndef setDomain(df):\n    df[p] = df[p].astype('str')\n    df[r] = df[r].astype('str')\n    \n    df[p] = df[p].fillna(unknown)\n    df[r] = df[r].fillna(unknown)\n    \n    df['email_check'] = np.where((df[p]==df[r])&(df[p]!=unknown),1,0)\n\n    df[p+'_prefix'] = df[p].apply(lambda x: x.split('.')[0])\n    df[r+'_prefix'] = df[r].apply(lambda x: x.split('.')[0])\n    \n    return df\n    \nX_train=setDomain(X_train)\nX_test=setDomain(X_test)","execution_count":11,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# TransactionDT degerlerinden icin yeni degiskenler uretilmis.\n\nimport datetime\nfrom pandas.tseries.holiday import USFederalHolidayCalendar as calendar\n\n# start='2017-10-01', end='2019-01-01 arasindaki tarihler listelenmis.\ndates_range = pd.date_range(start='2017-10-01', end='2019-01-01')\n\n# start='2017-10-01', end='2019-01-01 ABD ulusal tatil gunleri listelenmis. \nus_holidays = calendar().holidays(start=dates_range.min(), end=dates_range.max())\n\nSTART_DATE = '2017-12-01'\nstartdate = datetime.datetime.strptime(START_DATE, \"%Y-%m-%d\")\n\n# islemlerin yapildigi hour of day, day of week ve day of month ve month of year degiskeni olusturulmus.\n\nfor df in [X_train,X_test]:\n    \n    df[\"Date\"] = df['TransactionDT'].apply(lambda x: (startdate + datetime.timedelta(seconds=x)))\n    df['_Weekdays'] = df['Date'].dt.dayofweek\n    df['_Dayhours'] = df['Date'].dt.hour\n    df['_Monthdays'] = df['Date'].dt.day\n    df['_Yearmonths'] = (df['Date'].dt.month).astype(np.int8) \n\n    # yapilan islem tatil gunumu mu?\n\n    df['is_holiday'] = (df['Date'].dt.date.astype('datetime64').isin(us_holidays)).astype(np.int8)\n\n    # Timestamp tipinde olduğu için algoritma tanımlayamıyor.\n    df.drop(\"Date\", axis=1,inplace=True)","execution_count":12,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# ProductCD value_count = (W,C,R,H,S) \n# M4 value_count = (M0,M1,M2)\n\n# kategorik degisken olan ProductCD ve M4, 'fraud' ortalamalarina gore gruplandiriliyor\n\nfor col in ['ProductCD','M4']:\n    temp_dict = X_train.groupby([col])['isFraud'].agg(['mean']).reset_index().rename(columns={'mean': col+'_target_mean'})\n    temp_dict.index = temp_dict[col].values\n    temp_dict = temp_dict[col+'_target_mean'].to_dict()\n    \n    if col=='ProductCD':\n        X_train['ProductCD_1'] = X_train[col].map(temp_dict)\n        X_test['ProductCD_1']  = X_test[col].map(temp_dict)\n    else:\n        X_train['M4_1'] = X_train[col].map(temp_dict)\n        X_test['M4_1']  = X_test[col].map(temp_dict)\n        \n        \n# orjinal featurelar drop edilecek\n\nX_train.drop(['ProductCD','M4'], axis=1,inplace=True)\nX_test.drop(['ProductCD','M4'], axis=1,inplace=True)","execution_count":13,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# dolar kuruna gore ulke tahmini\n# kategorik olarak tutmamiz gerekiyor, one hot encoding yapilmasi gerekiyor\n\nfor df in [X_train,X_test]:\n    \n    df['TransactionAmt_decimal_lenght'] = df['TransactionAmt'].astype(str).str.split('.', expand=True)[1].str.len()\n    df['cents'] = (df['TransactionAmt'] - np.floor(X_train['TransactionAmt'])).astype('float32')","execution_count":14,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Modelde D ve D' nin normalize edilmiş  kolonlarının çıkarılmış halini de deneyeceğiz.\n# The D Columns are \"time deltas\" from some point in the past. We will transform the D Columns into their point in the past.\n# NORMALIZE D COLUMNS\nfor i in range(1,16):\n    if i in [1,2,3,5,9]:\n        continue\n    X_train['D'+str(i)] =  X_train['D'+str(i)] - X_train.TransactionDT/np.float32(24*60*60)\n    X_test['D'+str(i)] = X_test['D'+str(i)] - X_test.TransactionDT/np.float32(24*60*60)","execution_count":15,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for df in [X_train,X_test]:\n   \n    df = df.replace(np.inf,999)# sonsuz değerleri 999 ile değiştiriyor","execution_count":16,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"factorize_categoric()\n\nnum_positiv(X_train,X_test)\n\nclass_freq(['addr1','card1','card2','card3','P_emaildomain'])\n\naggreg(['TransactionAmt','D4','D9','D10','D11','D15'],'userid','mean')\n\naggreg(['TransactionAmt','D4','D9','D10','D11','D15','C14'],'userid','std')\n\naggreg(['C'+str(x) for x in range(1,15) if x!=3],'userid','mean')\n\naggreg(['M'+str(x) for x in range(1,10) if x!=4],'userid','mean')\n\naggreg_uniq(['P_emaildomain','dist1','id_02','cents','C13','V314','V127','V136','V309','V307','V320'],'userid')","execution_count":17,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.drop(\"userid\", axis=1, inplace=True)\nX_test.drop(\"userid\", axis=1, inplace=True)","execution_count":18,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def user_id(col1,col2):\n    us_id = col1+'_'+col2\n    \n    X_train['day'] = X_train.TransactionDT / (24*60*60)\n    X_train[us_id] = X_train[col1].astype(str)+'_'+X_train[col2].astype(str)+'_'+np.floor(X_train.day-X_train.D1).astype(str)\n\n    X_test['day'] = X_test.TransactionDT / (24*60*60)\n    X_test[us_id] = X_test[col1].astype(str)+'_'+X_test[col2].astype(str)+'_'+np.floor(X_test.day-X_test.D1).astype(str)","execution_count":19,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"categoric_features = ['card1','card2','card3','card4','card5','card6','addr1','addr2',\n                      'P_emaildomain','R_emaildomain',\n                      'M1','M2','M3','M5','M6','M7','M8','M9',\n                      'id_01','id_02','id_03','id_04','id_05','id_06','id_07','id_08','id_09','id_10','id_11','id_12','id_13','id_14','id_15','id_16','id_17','id_18','id_19',\n                      'id_20','id_21','id_22','id_23','id_24','id_25','id_26','id_27','id_28','id_29','id_30','id_31','id_32','id_33','id_34','id_35','id_36','id_37','id_38',\n                      'DeviceType','DeviceInfo',\n                      'OS_id_30','version_id_30','browser_id_31','version_id_31','Trans_min_std','lastest_browser','P_emaildomain_suffix','R_emaildomain_suffix','email_check',\n                      'P_emaildomain_prefix','R_emaildomain_prefix','_Weekdays','_Dayhours','_Monthdays','_Yearmonths','is_holiday','ProductCD_1','M4_1','TransactionAmt_decimal_lenght','cents']\n\ncategoric_features_index = [X_train.columns.get_loc(c) for c in categoric_features if c in X_train]","execution_count":20,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train = reduce_mem_usage(X_train)\nX_test = reduce_mem_usage(X_test)","execution_count":21,"outputs":[{"output_type":"stream","text":"Memory usage of dataframe is 497.02 MB\nMemory usage after optimization is: 361.85 MB\nDecreased by 27.2%\nMemory usage of dataframe is 431.70 MB\nMemory usage after optimization is: 315.73 MB\nDecreased by 26.9%\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.drop(\"isFraud\", axis=1, inplace=True)","execution_count":22,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Hyperparameters Tuning and Building Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train1, X_test1, y_train1, y_test1 = train_test_split(X_train, y_train, test_size=0.33, random_state=42)","execution_count":23,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import xgboost as xgb\nfrom sklearn import metrics\nfrom sklearn.model_selection import GridSearchCV\n\ndef auc(m, train, test,y_train1,y_test1): \n    return (metrics.roc_auc_score(y_train1,m.predict_proba(train)[:,1]),\n                            metrics.roc_auc_score(y_test1,m.predict_proba(test)[:,1]))\n\n# Parameter Tuning\nmodel = xgb.XGBClassifier()\nparam_dist = {\"max_depth\": [8,12,20],\n              \"min_child_weight\" : [1,3],\n              \"n_estimators\": [200,400],\n              \"learning_rate\": [0.14,0.2, 0.25],\n              \"nthread\":[4],\n              \"tree_method\":[\"gpu_hist\"],\n              \"random_state\": [2]}\n                             \ngrid_search = GridSearchCV(model, param_grid=param_dist, cv = 3, \n                                   verbose=10,n_jobs =-1)\n\ngrid_search.fit(X_train1, y_train1)","execution_count":24,"outputs":[{"output_type":"stream","text":"Fitting 3 folds for each of 36 candidates, totalling 108 fits\n","name":"stdout"},{"output_type":"stream","text":"[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n[Parallel(n_jobs=-1)]: Done   1 tasks      | elapsed:   28.2s\n[Parallel(n_jobs=-1)]: Done   4 tasks      | elapsed:  1.0min\n[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:  1.9min\n[Parallel(n_jobs=-1)]: Done  14 tasks      | elapsed:  3.2min\n[Parallel(n_jobs=-1)]: Done  21 tasks      | elapsed:  5.7min\n[Parallel(n_jobs=-1)]: Done  28 tasks      | elapsed: 10.0min\n[Parallel(n_jobs=-1)]: Done  37 tasks      | elapsed: 14.5min\n[Parallel(n_jobs=-1)]: Done  46 tasks      | elapsed: 16.6min\n[Parallel(n_jobs=-1)]: Done  57 tasks      | elapsed: 19.7min\n[Parallel(n_jobs=-1)]: Done  68 tasks      | elapsed: 25.2min\n[Parallel(n_jobs=-1)]: Done  81 tasks      | elapsed: 28.4min\n[Parallel(n_jobs=-1)]: Done  94 tasks      | elapsed: 32.3min\n[Parallel(n_jobs=-1)]: Done 108 out of 108 | elapsed: 38.0min finished\n","name":"stderr"},{"output_type":"execute_result","execution_count":24,"data":{"text/plain":"GridSearchCV(cv=3, error_score=nan,\n             estimator=XGBClassifier(base_score=None, booster=None,\n                                     colsample_bylevel=None,\n                                     colsample_bynode=None,\n                                     colsample_bytree=None, gamma=None,\n                                     gpu_id=None, importance_type='gain',\n                                     interaction_constraints=None,\n                                     learning_rate=None, max_delta_step=None,\n                                     max_depth=None, min_child_weight=None,\n                                     missing=nan, monotone_constraints=None,\n                                     n_estim...\n                                     subsample=None, tree_method=None,\n                                     validate_parameters=False,\n                                     verbosity=None),\n             iid='deprecated', n_jobs=-1,\n             param_grid={'learning_rate': [0.14, 0.2, 0.25],\n                         'max_depth': [8, 12, 20], 'min_child_weight': [1, 3],\n                         'n_estimators': [200, 400], 'nthread': [4],\n                         'random_state': [2], 'tree_method': ['gpu_hist']},\n             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n             scoring=None, verbose=10)"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"max_depth_best = grid_search.best_estimator_.max_depth\nn_estimators_best = grid_search.best_estimator_.n_estimators\nlearning_rate_best = grid_search.best_estimator_.learning_rate\nmin_child_weight_best = grid_search.best_estimator_.min_child_weight","execution_count":25,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del model \nx=gc.collect()\n","execution_count":26,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# MODEL AND PREDICTION\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"model = xgb.XGBClassifier(max_depth=max_depth_best,\n                          min_child_weight=min_child_weight_best, \n                          n_estimators=n_estimators_best,\n                          n_jobs=-1 , \n                          verbose=1,\n                          learning_rate=learning_rate_best,\n                          subsample=0.8,\n                          colsample_bytree=0.4,\n                          missing=-1,\n                          eval_metric='auc',\n                          # USE CPU\n                          nthread=4,\n                          #tree_method='hist'\n                          # USE GPU\n                          tree_method='gpu_hist')\n\n\nmodel.fit(X_train1,y_train1)","execution_count":28,"outputs":[{"output_type":"execute_result","execution_count":28,"data":{"text/plain":"XGBClassifier(base_score=0.5, booster=None, colsample_bylevel=1,\n              colsample_bynode=1, colsample_bytree=0.4, eval_metric='auc',\n              gamma=0, gpu_id=0, importance_type='gain',\n              interaction_constraints=None, learning_rate=0.25,\n              max_delta_step=0, max_depth=12, min_child_weight=1, missing=-1,\n              monotone_constraints=None, n_estimators=400, n_jobs=-1, nthread=4,\n              num_parallel_tree=1, objective='binary:logistic', random_state=0,\n              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=0.8,\n              tree_method='gpu_hist', validate_parameters=False, verbose=1,\n              verbosity=None)"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import metrics\npred1 = model.predict(X_test1)\nfpr, tpr, thresholds = metrics.roc_curve(y_test1, pred1, pos_label=2)\nmetrics.auc(fpr, tpr)\n\nprint(metrics.confusion_matrix(y_test1, pred1))\nprint(metrics.classification_report(y_test1, pred1))","execution_count":29,"outputs":[{"output_type":"stream","text":"/opt/conda/lib/python3.7/site-packages/sklearn/metrics/_ranking.py:808: UndefinedMetricWarning: No positive samples in y_true, true positive value should be meaningless\n  UndefinedMetricWarning)\n","name":"stderr"},{"output_type":"stream","text":"[[187745    188]\n [  2075   4871]]\n              precision    recall  f1-score   support\n\n           0       0.99      1.00      0.99    187933\n           1       0.96      0.70      0.81      6946\n\n    accuracy                           0.99    194879\n   macro avg       0.98      0.85      0.90    194879\nweighted avg       0.99      0.99      0.99    194879\n\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"preds = model.predict_proba(X_test)[:,1]","execution_count":30,"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'X_test_best' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-30-930b99b9f425>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test_best\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'X_test_best' is not defined"]}]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_submission = pd.read_csv('../input/ieee-fraud-detection/sample_submission.csv')\nsample_submission.isFraud = preds\nsample_submission.to_csv('sub_xgb1.csv',index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}