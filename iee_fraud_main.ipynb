{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IEEE FRAUD NOTEBOOK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm_notebook\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import gc\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "pd.set_option('max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_mem_usage(df):\n",
    "    \"\"\" iterate through all the columns of a dataframe and modify the data type\n",
    "        to reduce memory usage.        \n",
    "    \"\"\"\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n",
    "    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "        \n",
    "        if col_type != object:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "        else:\n",
    "            df[col] = df[col].astype('category')\n",
    "\n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n",
    "    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n",
    "    \n",
    "    return df\n",
    "\n",
    "def import_data(file):\n",
    "    \"\"\"create a dataframe and optimize its memory usage\"\"\"\n",
    "    df = pd.read_csv(file, parse_dates=True, keep_date_col=True, index_col='TransactionID')\n",
    "    df = reduce_mem_usage(df)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Memory usage of dataframe is 45.12 MB\n",
      "Memory usage after optimization is: 10.57 MB\n",
      "Decreased by 76.6%\n",
      "\tSuccessfully loaded train_identity!\n",
      "Memory usage of dataframe is 1775.15 MB\n",
      "Memory usage after optimization is: 489.41 MB\n",
      "Decreased by 72.4%\n",
      "\tSuccessfully loaded train_transaction!\n",
      "Memory usage of dataframe is 44.39 MB\n",
      "Memory usage after optimization is: 10.40 MB\n",
      "Decreased by 76.6%\n",
      "\tSuccessfully loaded test_identity!\n",
      "Memory usage of dataframe is 1519.24 MB\n",
      "Memory usage after optimization is: 427.17 MB\n",
      "Decreased by 71.9%\n",
      "\tSuccessfully loaded test_transaction!\n",
      "Data was successfully loaded!\n",
      "\n",
      "Wall time: 9min 53s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "print('Loading data...')\n",
    "\n",
    "train_identity = import_data('train_identity.csv')\n",
    "print('\\tSuccessfully loaded train_identity!')\n",
    "\n",
    "train_transaction = import_data('train_transaction.csv')\n",
    "print('\\tSuccessfully loaded train_transaction!')\n",
    "\n",
    "test_identity = import_data('test_identity.csv')\n",
    "print('\\tSuccessfully loaded test_identity!')\n",
    "\n",
    "test_transaction = import_data('test_transaction.csv')\n",
    "print('\\tSuccessfully loaded test_transaction!')\n",
    "\n",
    "\n",
    "test_identity.columns = train_identity.columns\n",
    "\n",
    "print('Data was successfully loaded!\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging data...\n",
      "Data was successfully merged!\n",
      "\n",
      "Train dataset has 590540 rows and 433 columns.\n",
      "Test dataset has 506691 rows and 432 columns.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "53"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Merging data...')\n",
    "train = train_transaction.merge(train_identity, how='left', left_index=True, right_index=True)\n",
    "test = test_transaction.merge(test_identity, how='left', left_index=True, right_index=True)\n",
    "\n",
    "# \"isFraud\" bagımlı degiskenini y_train degiskenine atadık\n",
    "y_train = train[\"isFraud\"]\n",
    "# Train bagimsiz degiskenleri\n",
    "df_train = train.drop(\"isFraud\", axis=1)\n",
    "\n",
    "print('Data was successfully merged!\\n')\n",
    "\n",
    "del train_identity, train_transaction, test_identity, test_transaction\n",
    "\n",
    "print(f'Train dataset has {train.shape[0]} rows and {train.shape[1]} columns.')\n",
    "print(f'Test dataset has {test.shape[0]} rows and {test.shape[1]} columns.')\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NOT: LÜTFEN YUKARIDAKİ HİÇBİR DEĞERİ VE DEĞİŞKENİ DEĞİŞTİRMEYİN.\n",
    "\n",
    "Örneğin train dosyasında değişiklik yapacaksanız aşağıdaki gibi copyasını alarak yapınız.\n",
    "df_train_copya = df_train.copy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Ozan\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Ümit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Muhammet(Alm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Ismail"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-) (card1 - card4 - addr1) ile (TransactionAmt - id_02 - D15) arasinda uretilen degiskenler\n",
    "\n",
    "- https://www.kaggle.com/davidcairuz/feature-engineering-lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_a = ['TransactionAmt', 'id_02', 'D15']\n",
    "columns_b = ['card1', 'card4', 'addr1']\n",
    "\n",
    "for col_a in columns_a:\n",
    "    for col_b in columns_b:\n",
    "        for df in [train, test]:\n",
    "            df[f'{col_a}_to_mean_{col_b}'] = df[col_a] / df.groupby([col_b])[col_a].transform('mean')\n",
    "            df[f'{col_a}_to_std_{col_b}'] = df[col_a] / df.groupby([col_b])[col_a].transform('std')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### card1 - TransactionAmt\n",
    "\n",
    "- card1 degiskeni onemli bir degisken, kullanici belirlemede ise yarayabilecek bir degisken, burada yapilan her bir harcamanin (transactionID) card1 bilgisi var ama bu card ile baska harcamalarda yapilmis, bu card ile yapilan toplam harcamalarin ortalamasini (std icin de yapiyor) belirliyor ve sadece bu harcamadaki miktar ile oranina bakiyor,\n",
    "\n",
    "- mesela x card, 3 farkli islemde kullanilmis ve her islemde 5, 10, 15 para birimlik harcama yapilmis, sonucta x card islem basina harcama ortalamasi 10\n",
    "\n",
    "- 1.islemde gercek harcama 5/ ortalama harcama 10\n",
    "- 2.islemde 10/10\n",
    "- 3.islemde 15/10 \n",
    "\n",
    "- bu sekilde her islemdeki card'in o islemdeki harcamasinin toplam card harcama ortalamasina oranini tespit ediyoruz\n",
    "\n",
    "- bu islem hem MEAN hem de STD icin ayri ayri yapiliyor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train['TransactionAmt'] / train.groupby('card1')['TransactionAmt'].transform('mean')).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### card4 - TransactionAmt\n",
    "\n",
    "- card4 kartin visa, mastercard, debit vs... olup olmadigini bilgisini tutuyor, burada da ayni yukarida oldugu gibi, her bir islemdeki tutarin genel ortalamaya oranini veriyor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train['TransactionAmt'] / train.groupby('card4')['TransactionAmt'].transform('mean')).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### addr1 - TransactionAmt\n",
    "- addr1 degiskeni zipcode bilgisi veriyor, yani ayni zipcode'dan yapilan islemlerin tamamindaki miktarin ortalamasi ile ayni zipcode'dan yapilan her bir islemin orani hesaplanmis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train['TransactionAmt'] / train.groupby('addr1')['TransactionAmt'].transform('mean')).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### card1 - id_02\n",
    "\n",
    "- card1'i grupluyor ve id_02 degerlerinin mean ve std'sini cikartiyor, \n",
    "- id_02 degeri 1'den baslayip yaklasik 1 milyona kadar artan, ortalamasi 175000 civari olan bir sayisal deger, ne ifade ettigini anlayamadim\n",
    "- id_02'yi anlayamadigim icin buradaki feature uretmenin de mantigini cozemedim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train['id_02'] / train.groupby('card1')['id_02'].transform('mean')).head(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### card4 - id_02  - addr1 - id_02\n",
    "- benzer sekilde id_02 degiskeni ile card4 ve addr1 arasindaki iliskiyi de anlayamadim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (card1 - D15), (card4 - D15), (addr1 - D15)\n",
    "- D15 degiskeni hakkinda bir kanaat olusmadigindan bu yeni feature olusturmanin mantigi da kavranamadi."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-) TransactionAmt'nin LOGORITMASI\n",
    "- gerek var mi?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['TransactionAmt_Log'] = np.log(train['TransactionAmt'])\n",
    "test['TransactionAmt_Log'] = np.log(test['TransactionAmt'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-) TransactionAmt'nin virgulden sonraki rakamlari\n",
    "- 5 ve 10'nun kati rakamlarin dolar cinsinden, digerlerinin ise farkli para birimi cinsinden olabilecegine dair uretilmis bir degisken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['TransactionAmt_decimal'] = ((train['TransactionAmt'] - train['TransactionAmt'].astype(int)) * 1000).astype(int)\n",
    "test['TransactionAmt_decimal'] = ((test['TransactionAmt'] - test['TransactionAmt'].astype(int)) * 1000).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4-) Zaman degiskeni TransactionDT kullanilarak islemin haftanin hangi gunu ve gunun hangi saati yapildigini tespit eden yeni degisken\n",
    "- zaman degiskeni varken buna gerek var mi?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['Transaction_day_of_week'] = np.floor((train['TransactionDT'] / (3600 * 24) - 1) % 7)\n",
    "test['Transaction_day_of_week'] = np.floor((test['TransactionDT'] / (3600 * 24) - 1) % 7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- gunun hangi saati gerceklestigini belirten yeni degisken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['Transaction_hour'] = np.floor(train['TransactionDT'] / 3600) % 24\n",
    "test['Transaction_hour'] = np.floor(test['TransactionDT'] / 3600) % 24"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5-) Some arbitrary features interaction olarak belirtilmis yani rasgele, farkli degiskenler arasinda string formata cevirerek birlestirme islemi\n",
    "- kullaniciyi belirlemeye yonelik isimize yarayacagini dusundugumuz degiskenler arasinda bu sekilde bir iliski kurmak mantikli olur mu? Mesela device bilgisi 'samsung' ile email domain bilgisini birlestirimek kisinin tespitine yonelik fayda saglar mi? Ayni anda hem device hem de domain bilgisi ayni olanlari filtrelemek gibi birsey oluyor aslinda, \n",
    "- card, device, addr degiskenleri ile yapilanlar mantikli olabilir mi?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for feature in ['id_02__id_20', 'id_02__D8', 'D11__DeviceInfo', 'DeviceInfo__P_emaildomain', 'P_emaildomain__C2', \n",
    "                'card2__dist1', 'card1__card5', 'card2__id_20', 'card5__P_emaildomain', 'addr1__card1']:\n",
    "\n",
    "    f1, f2 = feature.split('__')\n",
    "    train[feature] = train[f1].astype(str) + '_' + train[f2].astype(str)\n",
    "    test[feature] = test[f1].astype(str) + '_' + test[f2].astype(str)\n",
    "\n",
    "    le = LabelEncoder()\n",
    "    le.fit(list(train[feature].astype(str).values) + list(test[feature].astype(str).values))\n",
    "    train[feature] = le.transform(list(train[feature].astype(str).values))\n",
    "    test[feature] = le.transform(list(test[feature].astype(str).values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6-) Encoding - count encoding for both train and test\n",
    "- ornek verelim, 2987000 ID'nin card1 degeri = 13926\n",
    "- train setinde card1=13926'dan 43 tane var, test setinde de 13 tane var...\n",
    "- yeni 43+13 degerini hesaplayip card1=13926 olan butun ID'lerin oldugu satira bu degiskeni 56 olarak ekliyor\n",
    "- gereksiz bir degisken gibi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for feature in ['card1', 'card2', 'card3', 'card4', 'card5', 'card6', 'id_36']:\n",
    "    train[feature + '_count_full'] = train[feature].map(pd.concat([train[feature], test[feature]], ignore_index=True).value_counts(dropna=False))\n",
    "    test[feature + '_count_full'] = test[feature].map(pd.concat([train[feature], test[feature]], ignore_index=True).value_counts(dropna=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7-) Hangi ulkeden baglanti yapildigina dair ise yarayabilecek feature, _bin seklinde olana gerek olmayabilir, digeri mantikli gibi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "emails = {'gmail': 'google', 'att.net': 'att', 'twc.com': 'spectrum', 'scranton.edu': 'other', 'optonline.net': 'other', 'hotmail.co.uk': 'microsoft', \n",
    "          'comcast.net': 'other', 'yahoo.com.mx': 'yahoo', 'yahoo.fr': 'yahoo', 'yahoo.es': 'yahoo', 'charter.net': 'spectrum', 'live.com': 'microsoft', \n",
    "          'aim.com': 'aol', 'hotmail.de': 'microsoft', 'centurylink.net': 'centurylink', 'gmail.com': 'google', 'me.com': 'apple', 'earthlink.net': 'other', \n",
    "          'gmx.de': 'other', 'web.de': 'other', 'cfl.rr.com': 'other', 'hotmail.com': 'microsoft', 'protonmail.com': 'other', 'hotmail.fr': 'microsoft', \n",
    "          'windstream.net': 'other', 'outlook.es': 'microsoft', 'yahoo.co.jp': 'yahoo', 'yahoo.de': 'yahoo', 'servicios-ta.com': 'other', 'netzero.net': 'other', \n",
    "          'suddenlink.net': 'other', 'roadrunner.com': 'other', 'sc.rr.com': 'other', 'live.fr': 'microsoft', 'verizon.net': 'yahoo', 'msn.com': 'microsoft', \n",
    "          'q.com': 'centurylink', 'prodigy.net.mx': 'att', 'frontier.com': 'yahoo', 'anonymous.com': 'other', 'rocketmail.com': 'yahoo', 'sbcglobal.net': 'att', \n",
    "          'frontiernet.net': 'yahoo', 'ymail.com': 'yahoo', 'outlook.com': 'microsoft', 'mail.com': 'other', 'bellsouth.net': 'other', 'embarqmail.com': 'centurylink', \n",
    "          'cableone.net': 'other', 'hotmail.es': 'microsoft', 'mac.com': 'apple', 'yahoo.co.uk': 'yahoo', 'netzero.com': 'other', 'yahoo.com': 'yahoo', 'live.com.mx': 'microsoft', \n",
    "          'ptd.net': 'other', 'cox.net': 'other', 'aol.com': 'aol', 'juno.com': 'other', 'icloud.com': 'apple'}\n",
    "\n",
    "us_emails = ['gmail', 'net', 'edu']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in ['P_emaildomain', 'R_emaildomain']:\n",
    "    train[c + '_bin'] = train[c].map(emails)\n",
    "    test[c + '_bin'] = test[c].map(emails)\n",
    "    \n",
    "    train[c + '_suffix'] = train[c].map(lambda x: str(x).split('.')[-1])\n",
    "    test[c + '_suffix'] = test[c].map(lambda x: str(x).split('.')[-1])\n",
    "    \n",
    "    train[c + '_suffix'] = train[c + '_suffix'].map(lambda x: x if str(x) not in us_emails else 'us')\n",
    "    test[c + '_suffix'] = test[c + '_suffix'].map(lambda x: x if str(x) not in us_emails else 'us')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "com    466477\n",
       "us      25038\n",
       "mx       2499\n",
       "es        877\n",
       "de        506\n",
       "fr        494\n",
       "uk        161\n",
       "jp         32\n",
       "Name: P_emaildomain_suffix, dtype: int64"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['P_emaildomain_suffix'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "google         228851\n",
       "yahoo          109149\n",
       "microsoft       59477\n",
       "other           52868\n",
       "aol             28604\n",
       "apple            8225\n",
       "att              7210\n",
       "spectrum         1046\n",
       "centurylink       654\n",
       "Name: P_emaildomain_bin, dtype: int64"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['P_emaildomain_bin'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8-) TransactionAMT \n",
    "- https://www.kaggle.com/kabure/almost-complete-feature-engineering-ieee-data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- amt ile ilgili yukarida da benzer islemler yapildi ancak burada direk amt'nin degerinin ortalama degerinden farki ve std degerine bolunmesi\n",
    "- bana cok anlamli gelmeyen degiskenler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['Trans_min_mean'] = train['TransactionAmt'] - train['TransactionAmt'].mean()\n",
    "train['Trans_min_std'] = train['Trans_min_mean'] / train['TransactionAmt'].std()\n",
    "test['Trans_min_mean'] = test['TransactionAmt'] - test['TransactionAmt'].mean()\n",
    "test['Trans_min_std'] = test['Trans_min_mean'] / test['TransactionAmt'].std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9-) card'lar ile ilgili yapilanlar\n",
    "- https://www.kaggle.com/kabure/almost-complete-feature-engineering-ieee-data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "START_DATE = '2017-12-01'\n",
    "startdate = datetime.datetime.strptime(START_DATE, \"%Y-%m-%d\")\n",
    "train[\"Date\"] = train['TransactionDT'].apply(lambda x: (startdate + datetime.timedelta(seconds=x)))\n",
    "train['_Weekdays'] = train['Date'].dt.dayofweek\n",
    "train['_Hours'] = train['Date'].dt.hour\n",
    "train['_Days'] = train['Date'].dt.day\n",
    "\n",
    "test[\"Date\"] = test['TransactionDT'].apply(lambda x: (startdate + datetime.timedelta(seconds=x)))\n",
    "test['_Weekdays'] = test['Date'].dt.dayofweek\n",
    "test['_Hours'] = test['Date'].dt.hour\n",
    "test['_Days'] = test['Date'].dt.day"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- card (1,2,3,5) degiskenleri birlestirerek yeni bir degisken uretiyor,\n",
    "- mantikli olabilir cunku bunlari birlestirip kisilere ulasmak mumkun, bir nevi filtreleme yapip ayni degere sahip olanlari bulmak gibi dusunulebilir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corret_card_id(x): \n",
    "    x=x.replace('.0','')\n",
    "    x=x.replace('-999','nan')\n",
    "    return x\n",
    "\n",
    "# create card ID \n",
    "cards_cols= ['card1', 'card2', 'card3', 'card5']\n",
    "for card in cards_cols: \n",
    "    if '1' in card: \n",
    "        train['Card_ID']= train[card].map(str)\n",
    "        test['Card_ID']= test[card].map(str)\n",
    "    else : \n",
    "        train['Card_ID']+= ' '+train[card].map(str)\n",
    "        test['Card_ID']+= ' '+test[card].map(str)\n",
    "    \n",
    "# sort train data by Card_ID and then by transaction date \n",
    "train= train.sort_values(['Card_ID', 'Date'], ascending=[True, True])\n",
    "test= test.sort_values(['Card_ID', 'Date'], ascending=[True, True])\n",
    "    \n",
    "# small correction of the Card_ID\n",
    "train['Card_ID']=train['Card_ID'].apply(corret_card_id)\n",
    "test['Card_ID']=test['Card_ID'].apply(corret_card_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9500 321 150 226     14112\n",
       "15885 545 185 138    10332\n",
       "17188 321 150 226    10312\n",
       "7919 194 150 166      8844\n",
       "15066 170 150 102     7918\n",
       "Name: Card_ID, dtype: int64"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['Card_ID'].value_counts().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- yeni turetilen cardID degiskenine gore groupby islemleri yapiyor, benzerini ust tarafta ayri ayri yapmistik aslinda, rolling function ne oluyor tam olarak anlamayadim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['mean_last'] = train['TransactionAmt'] - train.groupby('Card_ID')['TransactionAmt'].transform(lambda x: x.rolling(10, 1).mean())\n",
    "train['min_last'] = train.groupby('Card_ID')['TransactionAmt'].transform(lambda x: x.rolling(10, 1).min())\n",
    "train['max_last'] = train.groupby('Card_ID')['TransactionAmt'].transform(lambda x: x.rolling(10, 1).max())\n",
    "train['std_last'] = train['mean_last'] / train.groupby('Card_ID')['TransactionAmt'].transform(lambda x: x.rolling(10, 1).std())\n",
    "\n",
    "# df['count_last'] = df.groupby('Card_ID')['TransactionAmt'].transform(lambda x: x.rolling(30, 1).count())\n",
    "train['mean_last'].fillna(0, inplace=True, )\n",
    "train['std_last'].fillna(0, inplace=True)\n",
    "\n",
    "test['mean_last'] = test['TransactionAmt'] - test.groupby('Card_ID')['TransactionAmt'].transform(lambda x: x.rolling(10, 1).mean())\n",
    "test['min_last'] = test.groupby('Card_ID')['TransactionAmt'].transform(lambda x: x.rolling(10, 1).min())\n",
    "test['max_last'] = test.groupby('Card_ID')['TransactionAmt'].transform(lambda x: x.rolling(10, 1).max())\n",
    "test['std_last'] = test['mean_last'] / test.groupby('Card_ID')['TransactionAmt'].transform(lambda x: x.rolling(10, 1).std())\n",
    "\n",
    "# df['count_last'] = df.groupby('Card_ID')['TransactionAmt'].transform(lambda x: x.rolling(30, 1).count())\n",
    "test['mean_last'].fillna(0, inplace=True, )\n",
    "test['std_last'].fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ayni islemleri bu kez rolling fonksiyonu kullanmadan yapiyor, bu iki islem arasindaki farki cozemedim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['TransactionAmt_to_mean_card_id'] = train['TransactionAmt'] - train.groupby(['Card_ID'])['TransactionAmt'].transform('mean')\n",
    "train['TransactionAmt_to_std_card_id'] = train['TransactionAmt_to_mean_card_id'] / train.groupby(['Card_ID'])['TransactionAmt'].transform('std')\n",
    "test['TransactionAmt_to_mean_card_id'] = test['TransactionAmt'] - test.groupby(['Card_ID'])['TransactionAmt'].transform('mean')\n",
    "test['TransactionAmt_to_std_card_id'] = test['TransactionAmt_to_mean_card_id'] / test.groupby(['Card_ID'])['TransactionAmt'].transform('std')\n",
    "\n",
    "train['TransactionAmt_to_mean_card1'] = train['TransactionAmt'] / train.groupby(['card1'])['TransactionAmt'].transform('mean')\n",
    "train['TransactionAmt_to_mean_card4'] = train['TransactionAmt'] / train.groupby(['card4'])['TransactionAmt'].transform('mean')\n",
    "train['TransactionAmt_to_std_card1'] = train['TransactionAmt'] / train.groupby(['card1'])['TransactionAmt'].transform('std')\n",
    "train['TransactionAmt_to_std_card4'] = train['TransactionAmt'] / train.groupby(['card4'])['TransactionAmt'].transform('std')\n",
    "\n",
    "test['TransactionAmt_to_mean_card1'] = test['TransactionAmt'] / test.groupby(['card1'])['TransactionAmt'].transform('mean')\n",
    "test['TransactionAmt_to_mean_card4'] = test['TransactionAmt'] / test.groupby(['card4'])['TransactionAmt'].transform('mean')\n",
    "test['TransactionAmt_to_std_card1'] = test['TransactionAmt'] / test.groupby(['card1'])['TransactionAmt'].transform('std')\n",
    "test['TransactionAmt_to_std_card4'] = test['TransactionAmt'] / test.groupby(['card4'])['TransactionAmt'].transform('std')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TransactionID\n",
       "3230924     0.000000\n",
       "3169988     0.000000\n",
       "3328484     0.000000\n",
       "3337343   -14.320312\n",
       "3337365    -4.000000\n",
       "Name: mean_last, dtype: float16"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['mean_last'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TransactionID\n",
       "3230924     0.000000\n",
       "3169988     0.000000\n",
       "3328484    17.312500\n",
       "3337343   -11.335938\n",
       "3337365    -3.000000\n",
       "Name: TransactionAmt_to_mean_card_id, dtype: float16"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['TransactionAmt_to_mean_card_id'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- card1 ve card2 nin ilk ve ilk iki degerleri, normalde kart numara bilgilerinde ilk rakamlar hangi kart tipi oldugunu belirtir ama buradaki numaralar onlara uymamakla birlikte hangi kart gibi oldugu bilgisi baska degiskende var, bu gereksi bir uretim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['first_value_card1'] = train['card1'].astype(str).str[0:1].astype(float)\n",
    "train['two_value_card1'] = train['card1'].astype(str).str[0:2].astype(float)\n",
    "\n",
    "test['first_value_card1'] = test['card1'].astype(str).str[0:1].astype(float)\n",
    "test['two_value_card1'] = test['card1'].astype(str).str[0:2].astype(float)\n",
    "\n",
    "train['card2'] = train['card2'].fillna(0)\n",
    "train['first_value_card2'] = train['card2'].astype(str).str[0:1].astype(float)\n",
    "train['two_value_card2'] = train['card2'].astype(str).str[0:2].astype(float)\n",
    "\n",
    "test['card2'] = test['card2'].fillna(0)\n",
    "test['first_value_card2'] = test['card2'].astype(str).str[0:1].astype(float)\n",
    "test['two_value_card2'] = test['card2'].astype(str).str[0:2].astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10-) adrr degiskenleri ile ilgili\n",
    "- adrr1 zipcode, adrr2 ulke kodu gibi dusundugumuzde ikisi arasinda toplama-cikarma islemi yapilmasinin bir mantigi olmadigini dusunuyorum\n",
    "- adrr1 yani zipcode degerlerinin birinci ve ilk iki rakaminin cikartilmasinin da gerekli olmadigini dusunuyorum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['diff_adrr'] = df_train.addr1 - df_train.addr2\n",
    "df_test['diff_adrr'] = df_test.addr1 - df_test.addr2\n",
    "\n",
    "df_train['diff_adrr_plus'] = df_train.addr1 + df_train.addr2\n",
    "df_test['diff_adrr_plus'] = df_test.addr1 + df_test.addr2\n",
    "\n",
    "df_train['first_value_addr1'] = df_train['addr1'].astype(str).str[0:1].astype(float)\n",
    "df_train['two_value_addr1'] = df_train['addr1'].astype(str).str[0:2].astype(float)\n",
    "\n",
    "df_test['first_value_addr1'] = df_test['addr1'].astype(str).str[0:1].astype(float)\n",
    "df_test['two_value_addr1'] = df_test['addr1'].astype(str).str[0:2].astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Berkan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Muhammet (Nor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LIGHT GBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBOOST  MODELI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
