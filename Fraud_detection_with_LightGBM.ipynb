{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\nimport gc\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**FUNCTIONS**","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"def reduce_mem_usage(df):\n    \"\"\" iterate through all the columns of a dataframe and modify the data type\n        to reduce memory usage.        \n    \"\"\"\n    start_mem = df.memory_usage().sum() / 1024**2\n    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n    \n    for col in df.columns:\n        col_type = df[col].dtype\n        \n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n        else:\n            df[col] = df[col].astype('category')\n\n    end_mem = df.memory_usage().sum() / 1024**2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n    \n    return df\n\ndef import_data(file):\n    \"\"\"create a dataframe and optimize its memory usage\"\"\"\n    df = pd.read_csv(file, parse_dates=True, keep_date_col=True, index_col='TransactionID')\n    df = reduce_mem_usage(df)\n    return df\n\n# AMAÇ: seçilen değişkenlerin uids lere göre gruplanıp ortalamaların alıp yeni değişkene atamak.\ndef aggreg(columns, userid, aggr='mean'):\n    \n    for col in columns:\n        new_col_name = col+'_'+userid+'_'+aggr # sectigi kolon isimlerini ve aggregation degerini birleştirip yeni değişken ismi oluşturmuş\n        df_temp = pd.concat([X_train[[userid, col]], X_test[[userid,col]]]) # Train ve Test setten seçtiği değişkenleri alt alta birleştirmiş.\n        df_temp.loc[df_temp[col]==-1,col] = np.nan # main_colums dan gelen değişkende -1 olan değerleri nan yapar.\n        # col değişkenine göre groupby atmış ve col değişkenine agg_type türüne göre işlem yapmış ve bunu new_col_name isimli yeni değişkene atamış.İNDEX İ DE SIFIRLADI.\n        df_temp = df_temp.groupby(userid)[col].agg([aggr]).reset_index().rename(columns={aggr: new_col_name})\n        df_temp.index = list(df_temp[userid]) # df_temp nin sıfırlanan indexleri col değişkeninin index leriyle değiştirildi.\n        df_temp = df_temp[new_col_name].to_dict()  # Yni oluşturulan değişken sözlük türüne çevirildi.\n        # Bu yeni değerler Train ve Test setlerinde ilgili kolona(col değişkeni) karşılık gelecek şekilde  \"new_col_name\" ismiyle Train ve Test sete eklendi.\n        X_train[new_col_name] = X_train[userid].map(df_temp).astype('float32')\n        X_test[new_col_name]  = X_test[userid].map(df_temp).astype('float32')\n        # Yeni olusturulan degiskenlerdeki nan değerler yerine -1 yazdırır.\n        X_train[new_col_name].fillna(-1,inplace=True)\n        X_test[new_col_name].fillna(-1,inplace=True)\n      \n\n # AMAÇ: uids lere göre main_colums daki değişkenler gruplanır ve ve bu main_column daki farklı değerler sayılır ve \n# bu toplam sayı Test ve Trainde ilgili \"TransactionID\" nin karşısına atanır.\ndef aggreg_uniq(columns, userid):\n    for col in columns:  \n        df = pd.concat([X_train[[userid,col]],X_test[[userid,col]]],axis=0)\n        #Burada uniq aynı col değişkenine denk gelen col değişkenindeki farklı değerlerin toplam sayısıdır.\n        uniq = df.groupby(userid)[col].agg(['nunique'])['nunique'].to_dict()\n\n        # \"uid_P_emaildomain_ct\" şeklinde değişken oluşturulur ve Train ile test setinde col değişkenindeki değerlere \"TransactionID\" baz alınarak uniq değerleri eşlenir.\n        X_train[col+'_count'] = X_train[userid].map(uniq).astype('float32')\n        X_test[col+'_count'] = X_test[userid].map(uniq).astype('float32')\n        \n# Her degeri minumum deger kadar arttiriyoruz, boylece hic eksi deger kalmiyor ve minumum deger 0 oluyor, bunu yapmadaki amac, NAN degere -1\n# verdigimizde ayri bir sinif olarak algilayabilmesi, cunku hic negatif deger kalmadi, sadece nan olanlar negatif olmus oldu\ndef num_positiv(X_train,X_test):\n    for f in X_train.columns:  \n        # Bütün nümerik değerleri pozitif yap ve NAN değerleri -1 yap. ['TransactionAmt','TransactionDT'] kolonları hariç.\n        if f not in ['TransactionAmt','TransactionDT',\"isFraud\"]: \n            mn = np.min((X_train[f].min(),X_test[f].min()))  # X_train ve X_test deki f kolonunun minimum degerlerini kıyasla ve en küçük olanını \"mn\" ye ata.\n\n            # Buradaki amaç bütün değerlerden en küçük değeri çıkartarak onları pozitif yaparken aralarındaki değer farkınıda korumaktır.\n            X_train[f] -= np.float32(mn)   # X_train deki f kolonundaki değerlerden mn yi yani en küçük değeri çıkarır.\n            X_test[f] -= np.float32(mn)    # X_test deki f kolonundaki değerlerden mn yi yani en küçük değeri çıkarır.\n            X_train[f].fillna(-1,inplace=True)  # X_train deki NaN değerleri -1 ile doldurur.\n            X_test[f].fillna(-1,inplace=True)   # X_test deki NaN değerleri -1 ile doldurur.\n            \n# AMAÇ: encode_FE fonksiyonu girilen data setlerindeki belirtilen kolonları normalize edip türlerini \"float32\" ye çevirip _FE uzantılı yeni bir değişken olarak data setlerine ekler.\ndef class_freq(cols):\n    for col in cols:\n        df = pd.concat([X_train[col],X_test[col]])\n        vc = df.value_counts(dropna=True).to_dict() # col. kolonundaki unique değerleri alıp bunları normalize ediyor ve listeye çevirip vc değişkeninde saklıyor.\n        vc[-1] = -1  # vc.  sözlüğüne -1 key adı ile -1 değerini ekliyor.\n        nm = col+'_freq' # kolon isimlerine uyguladığı FE encode uzantısını ekliyor.\n        X_train[nm] = X_train[col].map(vc)  #vc deki keys değerleri ile df1[col] daki index değerlerini eşleyip karşılığına vc deki values değerlerini atayıp bunu df1'e yeni değişken olarak atar.\n        X_test[nm] = X_test[col].map(vc)  #vc deki keys değerleri ile df2[col] daki index değerlerini eşleyip karşılığına vc deki values değerlerini atayıp bunu df2'ye yeni değişken olarak atar.\n        del df; x=gc.collect()\n        \n# butun categoruc (object) degiskenler icin factorize islemi yapildi,\n# factorize ile label encoder arasindaki en onemli fark, factorize nan degerleri -1 olarak tutuyor ama labelencoder da nan degerleri onceden doldurmak gerekiyor\ndef factorize_categoric():    \n    for col in X_train.select_dtypes(include=['category','object']).columns:\n        df = pd.concat([X_train[col],X_test[col]])\n        df,_ = df.factorize(sort=True)\n        X_train[col] = df[:len(X_train)].astype('int32')\n        X_test[col] = df[len(X_train):].astype('int32')\n        del df; x=gc.collect()        \n        \n# TEK BIR USERID OLUSTURMAK ICIN ONCELIKLE USERID OLUSTURMAK ICIN FAYDALI OLDUGUNU DEGERLENDIRDIGIMIZ FEATURELARI BELIRLEYIP\n# O FEATURLER ARASINDAN FARKLI KOMPINASYONLARI DENEYIP BIRDEN COK USERID OLUSTURALIM, DAHA SONRA BUNLARIN KOLERASYONUNA BAKIP\n# MANTIKLI OLAN HANGISI ISE ONU USERID OLARAK BELIRLEYELIM\n\ndef user_id(col1,col2):\n    us_id = col1+'_'+col2\n    \n    X_train[us_id] = X_train[col1].astype(str)+'_'+X_train[col2].astype(str)   # 12926.0_215.0, 3663.0_230.0  şeklinde çıktı üretir.\n    X_test[us_id] = X_test[col1].astype(str)+'_'+X_test[col2].astype(str)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\nprint('Loading data...')\n\ntrain_id = import_data(\"../input/ieee-fraud-detection/train_identity.csv\")\nprint('\\tSuccessfully loaded train_identity!')\n\nX_train = import_data('../input/ieee-fraud-detection/train_transaction.csv')\nprint('\\tSuccessfully loaded train_transaction!')\nX_train = X_train.merge(train_id, how='left', left_index=True, right_index=True) # Train setini kendi içinde merge etmiş\n\ntest_id = import_data('../input/ieee-fraud-detection/test_identity.csv')\nprint('\\tSuccessfully loaded test_identity!')\n\nX_test = import_data('../input/ieee-fraud-detection/test_transaction.csv')\nprint('\\tSuccessfully loaded test_transaction!')\n\ntest_id.columns = train_id.columns\nX_test = X_test.merge(test_id, how='left', left_index=True, right_index=True)  # Test setini kendi içinde merge etmiş\n\npd.set_option('max_columns', None)\n\n# TARGET\ny_train = X_train['isFraud'].copy()  # Train deki bağımlı değişkeni y_train setine atamış.\n\nprint('Data was successfully loaded!\\n')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Nan değerlere göre değişkenleri gruplama\nnan_groups={}\nv_cols = ['V'+str(i) for i in range(1,340)]\nfor i in X_train.columns:\n    nan_sum = X_train[i].isna().sum()\n    try:\n        nan_groups[nan_sum].append(i)\n    except:\n        nan_groups[nan_sum]=[i]\n\nfor i,j in nan_groups.items():\n    print('The Sum of the NaN Values =',i)\n    print(j)\n    \n    \n\nnon_group_list=list()\nfor i,j in nan_groups.items():\n    if len(j)>5:\n        if i != 0:\n            non_group_list.append(i)\n            \n            \n# Grupların kendi içinde korelasyon değeri 0.70 ten yüksek olan değişken grupları\n\n# D1 - V11 \ngrp1 = [[1],[2,3],[4,5],[6,7],[8,9],[10,11]]\n# V12 - V34\ngrp2 = [[12,13],[14],[15,16,17,18,21,22,31,32,33,34],[19,20],[23,24],[25,26],[27,28],[29,30]]\n# V35 - V52\ngrp3 = [[35,36],[37,38],[39,40,42,43,50,51,52],[41],[44,45],[46,47],[48,49]]\n# V53 - V74\ngrp4 = [[53,54],[55,56],[57,58,59,60,63,64,71,72,73,74],[61,62],[65],[66,67],[68],[69,70]]\n# V74 - V94\ngrp5 = [[75,76],[77,78],[79,80,81,84,85,92,93,94],[82,83],[86,87],[88],[89],[90,91]]\n# V95 - V107\ngrp6 = [[95,96,97,101,102,103,105,106],[98],[99,100],[104]]\n# V107 - V123\ngrp7 = [[107],[108,109,110,114],[111,112,113],[115,116],[117,118,119],[120,122],[121],[123]]\n# V124 - V137\ngrp8 = [[124,125],[126,127,128,132,133,134],[129],[130,131],[135,136,137]]\n# V138 - V163\ngrp9 = [[138],[139,140],[141,142],[146,147],[148,149,153,154,156,157,158],[161,162,163]]\n# V167 - V183\ngrp10 = [[167,168,177,178,179],[172,176],[173],[181,182,183]]\n# V184 - V216\ngrp11 = [[186,187,190,191,192,193,196,199],[202,203,204,211,212,213],[205,206],[207],[214,215,216]]\n# V217 - V238\ngrp12 = [[217,218,219,231,232,233,236,237],[223],[224,225],[226],[228],[229,230],[235]]\n# V240 - V262\ngrp13 = [[240,241],[242,243,244,258],[246,257],[247,248,249,253,254],[252],[260],[261,262]]\n# V263 - V278\ngrp14 = [[263,265,264],[266,269],[267,268],[273,274,275],[276,277,278]]\n# V220 - V272\ngrp15 = [[220],[221,222,227,245,255,256,259],[234],[238,239],[250,251],[270,271,272]]\n# V279 - V299\ngrp16 = [[279,280,293,294,295,298,299],[284],[285,287],[286],[290,291,292],[297]]\n# V302 - V321\ngrp17 = [[302,303,304],[305],[306,307,308,316,317,318],[309,311],[310,312],[319,320,321]]\n# V281 V315\ngrp18 = [[281],[282,283],[288,289],[296],[300,301],[313,314,315]]\n# V322 - V339\ngrp19 = [[322,323,324,326,327,328,329,330,331,332,333],[325],[334,335,336],[337,338,339]]\n\n\ngrp_list = [grp1,grp2,grp3,grp4,grp5,grp6,grp7,grp8,grp9,grp10,\n            grp11,grp12,grp13,grp14,grp15,grp16,grp17,grp18,grp19]\n\n\n\n\n# Aynı korelasyondaki değişkenlerden unique değeri fazla olanı seçen fonksiyon\ndef clip_group(group,df):\n    clipped_list = []\n    for i in group:\n        maximum = 0; \n        V_num = i[0]\n        for j in i:\n            n = df['V'+str(j)].value_counts().count()\n            if n>maximum:\n                maximum = n\n                V_num = j\n            \n        clipped_list.append(V_num)\n    \n        \n    print('Variables in the clipped_list: ',clipped_list)\n    return clipped_list\n\n\n# Korelasyon sonucunda modelde kullanılmasına karar verilen  V değişkenleri V_clipped_cols değişkeninde tutulmuştur.\nV_clipped_cols = list()\nfor i in grp_list:\n    for j in clip_group(i,X_train):\n        V_clipped_cols.append(\"V\"+str(j))\n        \n\nfor i in range (1, 339):\n    name = \"V\"+str(i)\n    if name not in V_clipped_cols:\n        X_train.drop(\"V\"+str(i),axis=1, inplace=True)\n        X_test.drop(\"V\"+str(i),axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# frekans sayısı 2 den az olan kartlar INVALID, çok olanlar VALID kart olarak tanimlaniyor, sonra train ve testin birinde bulunmuyorsa nan yapmış sonra invalid olanları nan yapmış\n\nvalid_card = pd.concat([X_train[['card1']], X_test[['card1']]])\nvalid_card = valid_card['card1'].value_counts()\nvalid_card_std = valid_card.values.std()\n\ninvalid_cards = valid_card[valid_card<=2]\n\nvalid_card = valid_card[valid_card>2]\nvalid_card = list(valid_card.index)\n\nX_train['card1'] = np.where(X_train['card1'].isin(X_test['card1']), X_train['card1'], np.nan)\nX_test['card1']  = np.where(X_test['card1'].isin(X_train['card1']), X_test['card1'], np.nan)\n\nX_train['card1'] = np.where(X_train['card1'].isin(valid_card), X_train['card1'], np.nan)\nX_test['card1']   = np.where(X_test['card1'].isin(valid_card), X_test['card1'], np.nan)\n\n# burda frekans sayısı 2 den az olan kartlara invalid çok olanlara valid kart demiş\n# sonra train ve testin ikisinde de bulunanları almış eğer birinde bulunmuyorsa nan yapmış\n# sonra invalid olanları nan yapmış\n\nfor col in ['card2','card3','card4','card5','card6']: \n    X_train[col] = np.where(X_train[col].isin(X_test[col]), X_train[col], np.nan)\n    X_test[col]  = np.where(X_test[col].isin(X_train[col]), X_test[col], np.nan)\n\n# train ve testin ikisinde de bulunanları almış eğer birinde bulunmuyorsa nan yapmış","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# USERID belirliyoruz\ncol_1 = 'card1'\ncol_2 = 'P_emaildomain'\ncol_3 = 'addr1'\n\n\nuser_id(col_1,col_2)\nuser_id(col_1+'_'+col_2,col_3)\nX_train.drop(col_1+'_'+col_2, axis = 1, inplace=True)\nX_test.drop(col_1+'_'+col_2, axis = 1, inplace=True)\n\nus_id = col_1 + '_' + col_2 + '_' + col_3\nX_train.rename(columns={us_id: 'userid'}, inplace=True)\nX_test.rename(columns={us_id: 'userid'}, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# cihaz ve browser tespitinin onemli oldugu varsayimiyla yapildi...\n\nfor df in [X_train,X_test]:\n\n    df['OS_id_30'] = df['id_30'].str.split(' ', expand=True)[0]\n    df['version_id_30'] = df['id_30'].str.split(' ', expand=True)[1]\n\n    df['browser_id_31'] = df['id_31'].str.split(' ', expand=True)[0]\n    df['version_id_31'] = df['id_31'].str.split(' ', expand=True)[1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# amt ilk halinde float16, bu sekilde std ve mean NAN oluyor, float32 yapmamiz lazim\n\nfor df in [X_train,X_test]:\n\n    df['TransactionAmt'] = df['TransactionAmt'].astype('float32')\n    df['Trans_min_std'] = (df['TransactionAmt'] - df['TransactionAmt'].mean()) / df['TransactionAmt'].std()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# lastest_browser (SON VERSIYON KONTROLU) son versiyon olanlar 1 \n\nX_train[\"lastest_browser\"] = np.zeros(X_train.shape[0])\nX_test[\"lastest_browser\"] = np.zeros(X_test.shape[0])\n\ndef setBrowser(df):\n    df.loc[df[\"id_31\"]==\"samsung browser 7.0\",'lastest_browser']=1\n    df.loc[df[\"id_31\"]==\"opera 53.0\",'lastest_browser']=1\n    df.loc[df[\"id_31\"]==\"mobile safari 10.0\",'lastest_browser']=1\n    df.loc[df[\"id_31\"]==\"google search application 49.0\",'lastest_browser']=1\n    df.loc[df[\"id_31\"]==\"firefox 60.0\",'lastest_browser']=1\n    df.loc[df[\"id_31\"]==\"edge 17.0\",'lastest_browser']=1\n    df.loc[df[\"id_31\"]==\"chrome 69.0\",'lastest_browser']=1\n    df.loc[df[\"id_31\"]==\"chrome 67.0 for android\",'lastest_browser']=1\n    df.loc[df[\"id_31\"]==\"chrome 63.0 for android\",'lastest_browser']=1\n    df.loc[df[\"id_31\"]==\"chrome 63.0 for ios\",'lastest_browser']=1\n    df.loc[df[\"id_31\"]==\"chrome 64.0\",'lastest_browser']=1\n    df.loc[df[\"id_31\"]==\"chrome 64.0 for android\",'lastest_browser']=1\n    df.loc[df[\"id_31\"]==\"chrome 64.0 for ios\",'lastest_browser']=1\n    df.loc[df[\"id_31\"]==\"chrome 65.0\",'lastest_browser']=1\n    df.loc[df[\"id_31\"]==\"chrome 65.0 for android\",'lastest_browser']=1\n    df.loc[df[\"id_31\"]==\"chrome 65.0 for ios\",'lastest_browser']=1\n    df.loc[df[\"id_31\"]==\"chrome 66.0\",'lastest_browser']=1\n    df.loc[df[\"id_31\"]==\"chrome 66.0 for android\",'lastest_browser']=1\n    df.loc[df[\"id_31\"]==\"chrome 66.0 for ios\",'lastest_browser']=1\n    return df\n\nX_train=setBrowser(X_train)\nX_test=setBrowser(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# MAIL ADRESLERININ SON KISIMLARINDAN ULKE TESPITINE YONELIK URETILEN FEATURE (com, us, mx, es, de, fr, uk, jp)\n\nus_emails = ['gmail', 'net', 'edu']\n\nfor df in [X_train,X_test]:\n    for c in ['P_emaildomain', 'R_emaildomain']:\n\n        df[c + '_suffix'] = df[c].map(lambda x: str(x).split('.')[-1])\n        df[c + '_suffix'] = df[c + '_suffix'].map(lambda x: x if str(x) not in us_emails else 'us')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Check if P_emaildomain matches R_emaildomain \n# extracts prefix\n\np = 'P_emaildomain'\nr = 'R_emaildomain'\nunknown = 'email_not_provided'\n\ndef setDomain(df):\n    df[p] = df[p].astype('str')\n    df[r] = df[r].astype('str')\n    \n    df[p] = df[p].fillna(unknown)\n    df[r] = df[r].fillna(unknown)\n    \n    df['email_check'] = np.where((df[p]==df[r])&(df[p]!=unknown),1,0)\n\n    df[p+'_prefix'] = df[p].apply(lambda x: x.split('.')[0])\n    df[r+'_prefix'] = df[r].apply(lambda x: x.split('.')[0])\n    \n    return df\n    \nX_train=setDomain(X_train)\nX_test=setDomain(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# TransactionDT degerlerinden icin yeni degiskenler uretilmis.\n\nimport datetime\nfrom pandas.tseries.holiday import USFederalHolidayCalendar as calendar\n\n# start='2017-10-01', end='2019-01-01 arasindaki tarihler listelenmis.\ndates_range = pd.date_range(start='2017-10-01', end='2019-01-01')\n\n# start='2017-10-01', end='2019-01-01 ABD ulusal tatil gunleri listelenmis. \nus_holidays = calendar().holidays(start=dates_range.min(), end=dates_range.max())\n\nSTART_DATE = '2017-12-01'\nstartdate = datetime.datetime.strptime(START_DATE, \"%Y-%m-%d\")\n\n# islemlerin yapildigi hour of day, day of week ve day of month ve month of year degiskeni olusturulmus.\n\nfor df in [X_train,X_test]:\n    \n    df[\"Date\"] = df['TransactionDT'].apply(lambda x: (startdate + datetime.timedelta(seconds=x)))\n    df['_Weekdays'] = df['Date'].dt.dayofweek\n    df['_Dayhours'] = df['Date'].dt.hour\n    df['_Monthdays'] = df['Date'].dt.day\n    df['_Yearmonths'] = (df['Date'].dt.month).astype(np.int8) \n\n    # yapilan islem tatil gunumu mu?\n\n    df['is_holiday'] = (df['Date'].dt.date.astype('datetime64').isin(us_holidays)).astype(np.int8)\n\n    # Timestamp tipinde olduğu için algoritma tanımlayamıyor.\n    df.drop(\"Date\", axis=1,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# ProductCD value_count = (W,C,R,H,S) \n# M4 value_count = (M0,M1,M2)\n\n# kategorik degisken olan ProductCD ve M4, 'fraud' ortalamalarina gore gruplandiriliyor\n\nfor col in ['ProductCD','M4']:\n    temp_dict = X_train.groupby([col])['isFraud'].agg(['mean']).reset_index().rename(columns={'mean': col+'_target_mean'})\n    temp_dict.index = temp_dict[col].values\n    temp_dict = temp_dict[col+'_target_mean'].to_dict()\n    \n    if col=='ProductCD':\n        X_train['ProductCD_1'] = X_train[col].map(temp_dict)\n        X_test['ProductCD_1']  = X_test[col].map(temp_dict)\n    else:\n        X_train['M4_1'] = X_train[col].map(temp_dict)\n        X_test['M4_1']  = X_test[col].map(temp_dict)\n        \n        \n# orjinal featurelar drop edilecek\n\nX_train.drop(['ProductCD','M4'], axis=1,inplace=True)\nX_test.drop(['ProductCD','M4'], axis=1,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# dolar kuruna gore ulke tahmini\n# kategorik olarak tutmamiz gerekiyor, one hot encoding yapilmasi gerekiyor\n\nfor df in [X_train,X_test]:\n    \n    df['TransactionAmt_decimal_lenght'] = df['TransactionAmt'].astype(str).str.split('.', expand=True)[1].str.len()\n    df['cents'] = (df['TransactionAmt'] - np.floor(X_train['TransactionAmt'])).astype('float32')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Modelde D ve D' nin normalize edilmiş  kolonlarının çıkarılmış halini de deneyeceğiz.\n# The D Columns are \"time deltas\" from some point in the past. We will transform the D Columns into their point in the past.\n# NORMALIZE D COLUMNS\nfor i in range(1,16):\n    if i in [1,2,3,5,9]:\n        continue\n    X_train['D'+str(i)] =  X_train['D'+str(i)] - X_train.TransactionDT/np.float32(24*60*60)\n    X_test['D'+str(i)] = X_test['D'+str(i)] - X_test.TransactionDT/np.float32(24*60*60)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for df in [X_train,X_test]:\n\n    df['mean_last'] = df['TransactionAmt'] - df.groupby('userid')['TransactionAmt'].transform(lambda x: x.rolling(10, 1).mean())\n    df['min_last'] = df.groupby('userid')['TransactionAmt'].transform(lambda x: x.rolling(10, 1).min())\n    df['max_last'] = df.groupby('userid')['TransactionAmt'].transform(lambda x: x.rolling(10, 1).max())\n    df['std_last'] = df['mean_last'] / df.groupby('userid')['TransactionAmt'].transform(lambda x: x.rolling(10, 1).std())\n\n    df['mean_last'].fillna(0, inplace=True, )\n    df['std_last'].fillna(0, inplace=True)\n\n    df['TransactionAmt_to_mean_card_id'] = df['TransactionAmt'] - df.groupby(['userid'])['TransactionAmt'].transform('mean')\n    df['TransactionAmt_to_std_card_id'] = df['TransactionAmt_to_mean_card_id'] / df.groupby(['userid'])['TransactionAmt'].transform('std')\n\n    df = df.replace(np.inf,999)# sonsuz değerleri 999 ile değiştiriyor","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"factorize_categoric()\n\nnum_positiv(X_train,X_test)\n\nclass_freq(['addr1','card1','card2','card3','P_emaildomain'])\n\naggreg(['TransactionAmt','D4','D9','D10','D11','D15'],'userid','mean')\n\naggreg(['TransactionAmt','D4','D9','D10','D11','D15','C14'],'userid','std')\n\naggreg(['C'+str(x) for x in range(1,15) if x!=3],'userid','mean')\n\naggreg(['M'+str(x) for x in range(1,10) if x!=4],'userid','mean')\n\naggreg_uniq(['P_emaildomain','dist1','id_02','cents','C13','V314','V127','V136','V309','V307','V320'],'userid')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.drop(\"userid\", axis=1, inplace=True)\nX_test.drop(\"userid\", axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def user_id(col1,col2):\n    us_id = col1+'_'+col2\n    \n    X_train['day'] = X_train.TransactionDT / (24*60*60)\n    X_train[us_id] = X_train[col1].astype(str)+'_'+X_train[col2].astype(str)+'_'+np.floor(X_train.day-X_train.D1).astype(str)\n\n    X_test['day'] = X_test.TransactionDT / (24*60*60)\n    X_test[us_id] = X_test[col1].astype(str)+'_'+X_test[col2].astype(str)+'_'+np.floor(X_test.day-X_test.D1).astype(str)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"categoric_features = ['card1','card2','card3','card4','card5','card6','addr1','addr2',\n                      'P_emaildomain','R_emaildomain',\n                      'M1','M2','M3','M5','M6','M7','M8','M9',\n                      'id_01','id_02','id_03','id_04','id_05','id_06','id_07','id_08','id_09','id_10','id_11','id_12','id_13','id_14','id_15','id_16','id_17','id_18','id_19',\n                      'id_20','id_21','id_22','id_23','id_24','id_25','id_26','id_27','id_28','id_29','id_30','id_31','id_32','id_33','id_34','id_35','id_36','id_37','id_38',\n                      'DeviceType','DeviceInfo',\n                      'OS_id_30','version_id_30','browser_id_31','version_id_31','Trans_min_std','lastest_browser','P_emaildomain_suffix','R_emaildomain_suffix','email_check',\n                      'P_emaildomain_prefix','R_emaildomain_prefix','_Weekdays','_Dayhours','_Monthdays','_Yearmonths','is_holiday','ProductCD_1','M4_1','TransactionAmt_decimal_lenght','cents']\n\ncategoric_features_index = [X_train.columns.get_loc(c) for c in categoric_features if c in X_train]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train = reduce_mem_usage(X_train)\nX_test = reduce_mem_usage(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.drop(\"isFraud\", axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# MODEL KISMI","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train1, X_test1, y_train1, y_test1 = train_test_split(X_train, y_train, test_size=0.33, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Tuning the Hyperparameters","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import lightgbm as lgb\nfrom sklearn import metrics\nfrom sklearn.model_selection import GridSearchCV\n\ndef auc(m, train, test,y_train1,y_test1): \n    return (metrics.roc_auc_score(y_train1,m.predict_proba(train)[:,1]),\n                            metrics.roc_auc_score(y_test1,m.predict_proba(test)[:,1]))\n\n# Parameter Tuning\nmodel = lgb.LGBMClassifier()\nparam_dist = {\"max_depth\": [5,10,15],\n              \"learning_rate\" : [0.1,0.15,0.3],\n              \"num_leaves\": [32,150,200],\n              \"n_estimators\": [300,400],\n              'is_unbalance': [True],\n              'boost_from_average': [False],\n              'device': ['gpu'],\n              'gpu_platform_id': [0],\n              'gpu_device_id': [0],\n              \"random_state\": [2]}\n\n\ngrid_search = GridSearchCV(model, param_grid=param_dist, cv = 3, \n                                   verbose=10, n_jobs=-1)\n\n\ngrid_search.fit(X_train1, y_train1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"max_depth_best = grid_search.best_estimator_.max_depth\nnum_leaves_best = grid_search.best_estimator_.num_leaves\nn_estimators_best = grid_search.best_estimator_.n_estimators\nlearning_rate_best = grid_search.best_estimator_.learning_rate","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# MODEL AND PREDICTION","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"model = lgb.LGBMClassifier(max_depth = max_depth_best,\n                          num_leaves = num_leaves_best, \n                          n_estimators = n_estimators_best,\n                          n_jobs = -1 , \n                          verbose = 1,\n                          learning_rate= learning_rate_best,\n                          eval_metric='auc',\n                          # USE CPU\n                          nthread=4,\n                          is_unbalance = True,\n                          boost_from_average = False,\n                          device = 'gpu',\n                          gpu_platform_id = 0,\n                          gpu_device_id = 0\n                          )\n                         \n                          \nmodel.fit(X_train1,y_train1)\n\n\n\nfrom sklearn import metrics\npred1 = model.predict(X_test1)\nfpr, tpr, thresholds = metrics.roc_curve(y_test1, pred1, pos_label=2)\nmetrics.auc(fpr, tpr)\n\nprint(metrics.confusion_matrix(y_test1, pred1))\nprint(metrics.classification_report(y_test1, pred1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"preds = model.predict_proba(X_test)[:,1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nsample_submission = pd.read_csv('../input/ieee-fraud-detection/sample_submission.csv')\nsample_submission.isFraud = preds\nsample_submission.to_csv('sub_lgbm.csv',index=False)\n\nplt.hist(sample_submission.isFraud,bins=100)\nplt.ylim((0,5000))\nplt.title('LightGBM Submission')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}